{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KUx All-in-One Colab Notebook\n",
    "\n",
    "Run the entire KUx workflow\u2014from environment setup to the multimodal chatbot\u2014in a single place. Execute the cells sequentially (or rerun specific sections) to rebuild the RAG store, optionally fine-tune Qwen3-Omni, and launch the Gradio demo on Google Colab Pro+.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** Adjust the configuration cell below before running the notebook. Every subsequent step consumes the values you set there, so you can quickly toggle crawling, ingestion, fine-tuning, or change model defaults without editing later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master configuration for the KUx workflow. Update values as needed before running the rest of the notebook.\n",
    "CONFIG = {\n",
    "    \"repo_url\": \"https://github.com/themistymoon/KUx.git\",\n",
    "    \"repo_dir\": \"/content/KUx\",\n",
    "    # Data collection toggles\n",
    "    \"enable_crawl\": False,\n",
    "    \"crawl_seed_urls\": [\n",
    "        \"https://cs.sci.ku.ac.th/\",\n",
    "    ],\n",
    "    \"crawl_max_depth\": 1,\n",
    "    \"crawl_max_pages\": 10,\n",
    "    # Document ingestion\n",
    "    \"enable_ingest\": True,\n",
    "    \"ingest_sources\": [\n",
    "        \"data/sample_documents\",\n",
    "        \"data/crawled\",\n",
    "    ],\n",
    "    # Optional LoRA fine-tuning\n",
    "    \"enable_finetune\": False,\n",
    "    \"finetune_dataset\": \"data/train.jsonl\",\n",
    "    \"finetune_epochs\": 2,\n",
    "    # Chatbot defaults\n",
    "    \"default_model_key\": \"qwen3-omni-30b\",\n",
    "    \"default_system_prompt\": \"\",\n",
    "    \"launch_share\": True,\n",
    "    \"launch_preload\": True,\n",
    "    # Storage locations (relative to the repo root)\n",
    "    \"vector_db_dir\": \"storage/vectorstore\",\n",
    "    \"adapter_dir\": \"outputs/finetuned-qwen\",\n",
    "}\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify the runtime GPU\n",
    "\n",
    "Confirm that the Colab session is attached to an **A100 80\u202fGB**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone the KUx repository\n",
    "\n",
    "Change `repo_url` in the configuration cell if you are working from a fork. The cell is idempotent\u2014it only clones when the target directory is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "REPO_URL = CONFIG[\"repo_url\"]\n",
    "TARGET_DIR = Path(CONFIG[\"repo_dir\"])\n",
    "\n",
    "if not TARGET_DIR.exists():\n",
    "    TARGET_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    !git clone {REPO_URL} {TARGET_DIR}\n",
    "\n",
    "ioshell = get_ipython()\n",
    "ioshell.run_line_magic('cd', str(TARGET_DIR))\n",
    "print('Working directory:', Path.cwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Python dependencies\n",
    "\n",
    "Install the runtime packages required for crawling, RAG ingestion, fine-tuning, and the chatbot UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Optional:**\n",
    ">\n",
    "> * Authenticate with Hugging Face if the Qwen model is gated.\n",
    "> * Mount Google Drive to persist vector stores (`storage/vectorstore/`) or LoRA adapters (`outputs/`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure project paths\n",
    "\n",
    "These directories are created automatically inside the cloned repository. Adjust the configuration cell if you want to point at a different location (e.g., a Drive mount).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "VECTOR_DB_DIR = PROJECT_ROOT / CONFIG['vector_db_dir']\n",
    "ADAPTER_DIR = PROJECT_ROOT / CONFIG['adapter_dir']\n",
    "\n",
    "for path in (DATA_DIR, VECTOR_DB_DIR, ADAPTER_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG['project_root'] = str(PROJECT_ROOT)\n",
    "CONFIG['data_dir'] = str(DATA_DIR)\n",
    "CONFIG['vector_db_dir'] = str(VECTOR_DB_DIR)\n",
    "CONFIG['adapter_dir'] = str(ADAPTER_DIR)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Data dir:', DATA_DIR)\n",
    "print('Vector DB dir:', VECTOR_DB_DIR)\n",
    "print('Adapter dir:', ADAPTER_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample documents bundled with KUx\n",
    "\n",
    "Use these starter files to test the pipeline before adding your own PDFs/CSVs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sorted((DATA_DIR / 'sample_documents').glob('*')):\n",
    "    size_kb = path.stat().st_size / 1024\n",
    "    print(f\"{path.relative_to(PROJECT_ROOT)} \u2014 {size_kb:.1f} KiB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KUx core logic within this notebook\n",
    "\n",
    "The next cells inline the configuration dataclasses, crawler, RAG ingestion, multimodal pipeline, fine-tuning helper, and chatbot UI so you can run everything directly without importing the `kux` Python package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Central configuration dataclasses for KUx project.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelOption:\n",
    "    \"\"\"Definition for a selectable base model in the chatbot UI.\"\"\"\n",
    "\n",
    "    key: str\n",
    "    label: str\n",
    "    model_name: str\n",
    "    multimodal: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"Configuration for supervised fine-tuning of Qwen.\"\"\"\n",
    "\n",
    "    model_name: str = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n",
    "    dataset_path: str = \"data/train.jsonl\"\n",
    "    output_dir: str = \"outputs/finetuned-qwen\"\n",
    "    learning_rate: float = 2e-4\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    max_seq_length: int = 4096\n",
    "    warmup_ratio: float = 0.03\n",
    "    weight_decay: float = 0.0\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 200\n",
    "    eval_steps: Optional[int] = None\n",
    "    seed: int = 42\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    load_in_4bit: bool = True\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    bf16: bool = True\n",
    "    dataset_text_field: str = \"text\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for retrieval augmented generation.\"\"\"\n",
    "\n",
    "    vector_db_path: Path = Path(\"storage/vectorstore\")\n",
    "    chunk_size: int = 1024\n",
    "    chunk_overlap: int = 80\n",
    "    allowed_document_types: List[str] = field(default_factory=lambda: [\".pdf\", \".csv\", \".txt\"])\n",
    "    embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    collection_name: str = \"kasetsart_documents\"\n",
    "    max_retrieval_docs: int = 6\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CrawlerConfig:\n",
    "    \"\"\"Configuration for crawling approved Kasetsart CS resources.\"\"\"\n",
    "\n",
    "    allowed_domains: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"www.ku.ac.th\",\n",
    "            \"www.cs.ku.ac.th\",\n",
    "            \"cs.ku.ac.th\",\n",
    "            \"registrar.ku.ac.th\",\n",
    "            \"admission.ku.ac.th\",\n",
    "        ]\n",
    "    )\n",
    "    user_agent: str = \"KUxBot/1.0 (+https://www.cs.ku.ac.th)\"\n",
    "    request_timeout: int = 20\n",
    "    max_depth: int = 1\n",
    "    max_pages: int = 20\n",
    "    cache_dir: Path = Path(\"storage/crawler_cache\")\n",
    "\n",
    "\n",
    "MODEL_OPTIONS: List[ModelOption] = [\n",
    "    ModelOption(\n",
    "        key=\"qwen3-omni-30b\",\n",
    "        label=\"Qwen3-Omni-30B-A3B-Instruct (multimodal)\",\n",
    "        model_name=\"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n",
    "        multimodal=True,\n",
    "    ),\n",
    "    ModelOption(\n",
    "        key=\"gpt-oss-120b\",\n",
    "        label=\"gpt-oss-120b (text-only)\",\n",
    "        model_name=\"Qwen/gpt-oss-120b\",\n",
    "        multimodal=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"TrainConfig\",\n",
    "    \"RAGConfig\",\n",
    "    \"CrawlerConfig\",\n",
    "    \"ModelOption\",\n",
    "    \"MODEL_OPTIONS\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple focused crawler for Kasetsart University domains.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import logging\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Set\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SiteCrawler:\n",
    "    \"\"\"Breadth-first crawler constrained to approved Kasetsart domains.\"\"\"\n",
    "\n",
    "    def __init__(self, config: CrawlerConfig | None = None) -> None:\n",
    "        self.config = config or CrawlerConfig()\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"User-Agent\": self.config.user_agent})\n",
    "        self.cache_dir = Path(self.config.cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Fetching helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    def _is_allowed(self, url: str) -> bool:\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain in self.config.allowed_domains\n",
    "\n",
    "    def _cache_path(self, url: str) -> Path:\n",
    "        digest = hashlib.sha256(url.encode(\"utf-8\")).hexdigest()\n",
    "        return self.cache_dir / f\"{digest}.html\"\n",
    "\n",
    "    def fetch(self, url: str) -> str:\n",
    "        if not self._is_allowed(url):\n",
    "            raise ValueError(f\"URL domain not allowed: {url}\")\n",
    "        cache_path = self._cache_path(url)\n",
    "        if cache_path.exists():\n",
    "            return cache_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        response = self.session.get(url, timeout=self.config.request_timeout)\n",
    "        response.raise_for_status()\n",
    "        cache_path.write_text(response.text, encoding=\"utf-8\")\n",
    "        return response.text\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Parsing helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    def extract_text(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        text = \"\\n\".join(chunk.strip() for chunk in soup.stripped_strings)\n",
    "        return text\n",
    "\n",
    "    def extract_links(self, base_url: str, html: str) -> List[str]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links: List[str] = []\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = tag[\"href\"]\n",
    "            absolute = urljoin(base_url, href)\n",
    "            if self._is_allowed(absolute):\n",
    "                links.append(absolute)\n",
    "        return links\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Crawling orchestration\n",
    "    # ------------------------------------------------------------------\n",
    "    def crawl(self, seeds: Iterable[str]) -> Dict[str, str]:\n",
    "        \"\"\"Crawl starting from the seed URLs and return url->text mapping.\"\"\"\n",
    "\n",
    "        visited: Set[str] = set()\n",
    "        queue: deque[tuple[str, int]] = deque((seed, 0) for seed in seeds)\n",
    "        results: Dict[str, str] = {}\n",
    "        while queue and len(results) < self.config.max_pages:\n",
    "            url, depth = queue.popleft()\n",
    "            if url in visited or depth > self.config.max_depth:\n",
    "                continue\n",
    "            try:\n",
    "                html = self.fetch(url)\n",
    "                text = self.extract_text(html)\n",
    "            except Exception as exc:  # pragma: no cover - network issues\n",
    "                LOGGER.warning(\"Failed to crawl %s: %s\", url, exc)\n",
    "                continue\n",
    "            visited.add(url)\n",
    "            results[url] = text\n",
    "            if depth < self.config.max_depth:\n",
    "                for link in self.extract_links(url, html):\n",
    "                    if link not in visited:\n",
    "                        queue.append((link, depth + 1))\n",
    "        return results\n",
    "\n",
    "\n",
    "__all__ = [\"SiteCrawler\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Document ingestion utilities for KUx retrieval augmented generation.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import CSVLoader, PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DocumentIngestor:\n",
    "    \"\"\"Ingest PDFs, CSVs and text files into a FAISS vector store.\"\"\"\n",
    "\n",
    "    def __init__(self, config: RAGConfig | None = None) -> None:\n",
    "        self.config = config or RAGConfig()\n",
    "        self.vector_db_path = Path(self.config.vector_db_path)\n",
    "        self.vector_db_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.config.embedding_model_name)\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config.chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap,\n",
    "        )\n",
    "\n",
    "    def _resolve_loader(self, file_path: Path):\n",
    "        suffix = file_path.suffix.lower()\n",
    "        if suffix == \".pdf\":\n",
    "            return PyPDFLoader(str(file_path))\n",
    "        if suffix == \".csv\":\n",
    "            return CSVLoader(str(file_path))\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return TextLoader(str(file_path))\n",
    "        raise ValueError(f\"Unsupported file type for ingestion: {suffix}\")\n",
    "\n",
    "    def _load_documents(self, path: Path):\n",
    "        loader = self._resolve_loader(path)\n",
    "        documents = loader.load()\n",
    "        LOGGER.info(\"Loaded %s documents from %s\", len(documents), path)\n",
    "        return documents\n",
    "\n",
    "    def ingest(self, sources: Iterable[str]) -> FAISS:\n",
    "        \"\"\"Ingest the provided sources into the FAISS vector store.\"\"\"\n",
    "\n",
    "        docs = []\n",
    "        for source in sources:\n",
    "            path = Path(source)\n",
    "            if path.is_dir():\n",
    "                for child in path.rglob(\"*\"):\n",
    "                    if child.suffix.lower() in self.config.allowed_document_types:\n",
    "                        docs.extend(self._load_documents(child))\n",
    "            elif path.suffix.lower() in self.config.allowed_document_types:\n",
    "                docs.extend(self._load_documents(path))\n",
    "            else:\n",
    "                LOGGER.warning(\"Skipping unsupported file: %s\", path)\n",
    "        if not docs:\n",
    "            raise RuntimeError(\"No documents were ingested. Check your source paths.\")\n",
    "\n",
    "        LOGGER.info(\"Splitting %s documents into chunks\", len(docs))\n",
    "        chunks = self.splitter.split_documents(docs)\n",
    "        LOGGER.info(\"Creating vector store with %s chunks\", len(chunks))\n",
    "\n",
    "        vector_store = FAISS.from_documents(chunks, embedding=self.embeddings)\n",
    "        vector_store.save_local(str(self.vector_db_path))\n",
    "        LOGGER.info(\"Vector store saved to %s\", self.vector_db_path)\n",
    "        return vector_store\n",
    "\n",
    "\n",
    "__all__ = [\"DocumentIngestor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieval augmented generation pipeline for KUx.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = dedent(\n",
    "    \"\"\"\n",
    "    You are KUx, an omniscient assistant for Kasetsart University Computer Science students.\n",
    "    Answer with verified facts from Kasetsart University sources. If unsure, state that you do not know.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "MODEL_OPTION_MAP = {option.key: option for option in MODEL_OPTIONS}\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MediaInput:\n",
    "    \"\"\"Container for user-provided multimodal attachments.\"\"\"\n",
    "\n",
    "    images: List[str] = field(default_factory=list)\n",
    "    audio: List[str] = field(default_factory=list)\n",
    "    video: List[str] = field(default_factory=list)\n",
    "\n",
    "    @classmethod\n",
    "    def from_payload(\n",
    "        cls,\n",
    "        images: Optional[Sequence[str]] = None,\n",
    "        audio: Optional[Sequence[str]] = None,\n",
    "        video: Optional[Sequence[str]] = None,\n",
    "    ) -> \"MediaInput\":\n",
    "        def _clean(items: Optional[Sequence[str]]) -> List[str]:\n",
    "            if not items:\n",
    "                return []\n",
    "            return [item for item in items if item]\n",
    "\n",
    "        return cls(images=_clean(images), audio=_clean(audio), video=_clean(video))\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        return not (self.images or self.audio or self.video)\n",
    "\n",
    "\n",
    "class LocalHFGenerator:\n",
    "    \"\"\"Wrapper around a local Hugging Face causal LM for inference.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        max_new_tokens: int = 1024,\n",
    "        temperature: float = 0.2,\n",
    "        adapter_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        LOGGER.info(\"Loading generator from %s\", model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = self._maybe_apply_adapter(base_model, adapter_path)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_apply_adapter(model: AutoModelForCausalLM, adapter_path: Optional[str]):\n",
    "        if not adapter_path:\n",
    "            return model\n",
    "        adapter_dir = Path(adapter_path)\n",
    "        if not adapter_dir.exists():\n",
    "            LOGGER.warning(\"Adapter path %s does not exist; continuing with the base model.\", adapter_dir)\n",
    "            return model\n",
    "        try:  # pragma: no cover - requires peft at runtime\n",
    "            from peft import PeftModel\n",
    "        except ImportError as exc:  # pragma: no cover\n",
    "            raise ImportError(\n",
    "                \"peft is required to load LoRA adapters. Install with `pip install peft`.\"\n",
    "            ) from exc\n",
    "\n",
    "        LOGGER.info(\"Applying LoRA adapters from %s\", adapter_dir)\n",
    "        adapted_model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "        return adapted_model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        outputs = self.pipe(prompt)\n",
    "        text = outputs[0][\"generated_text\"]\n",
    "        return text[len(prompt) :].strip()\n",
    "\n",
    "\n",
    "class QwenOmniGenerator:\n",
    "    \"\"\"Inference helper for Qwen3-Omni multimodal dialogue.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        max_new_tokens: int = 2048,\n",
    "        temperature: float = 0.2,\n",
    "        use_audio_in_video: bool = True,\n",
    "        adapter_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        try:  # pragma: no cover - heavy dependency only available at runtime\n",
    "            from transformers import (\n",
    "                Qwen3OmniMoeForConditionalGeneration,\n",
    "                Qwen3OmniMoeProcessor,\n",
    "            )\n",
    "        except ImportError as exc:  # pragma: no cover - import validated in runtime environment\n",
    "            raise ImportError(\n",
    "                \"Qwen3-Omni dependencies are missing. Install the latest transformers from \"\n",
    "                \"source (pip install git+https://github.com/huggingface/transformers).\"\n",
    "            ) from exc\n",
    "\n",
    "        LOGGER.info(\"Loading Qwen3-Omni model from %s\", model_path)\n",
    "        base_model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        if hasattr(base_model, \"disable_talker\"):\n",
    "            base_model.disable_talker()\n",
    "        self.model = self._maybe_apply_adapter(base_model, adapter_path)\n",
    "        self.processor = Qwen3OmniMoeProcessor.from_pretrained(model_path)\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.use_audio_in_video = use_audio_in_video\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_apply_adapter(model, adapter_path: Optional[str]):\n",
    "        if not adapter_path:\n",
    "            return model\n",
    "        adapter_dir = Path(adapter_path)\n",
    "        if not adapter_dir.exists():\n",
    "            LOGGER.warning(\"Adapter path %s does not exist; continuing with the base model.\", adapter_dir)\n",
    "            return model\n",
    "        try:  # pragma: no cover - requires peft during runtime\n",
    "            from peft import PeftModel\n",
    "        except ImportError as exc:  # pragma: no cover\n",
    "            raise ImportError(\n",
    "                \"peft is required to load Qwen3-Omni LoRA adapters. Install with `pip install peft`.\"\n",
    "            ) from exc\n",
    "\n",
    "        LOGGER.info(\"Applying Qwen3-Omni LoRA adapters from %s\", adapter_dir)\n",
    "        adapted_model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "        return adapted_model\n",
    "\n",
    "    @staticmethod\n",
    "    def _collect_media(messages: Sequence[dict]) -> tuple[Optional[List[str]], Optional[List[str]], Optional[List[str]]]:\n",
    "        images: List[str] = []\n",
    "        audio: List[str] = []\n",
    "        videos: List[str] = []\n",
    "        for message in messages:\n",
    "            content = message.get(\"content\", [])\n",
    "            if isinstance(content, dict):\n",
    "                content = [content]\n",
    "            for item in content:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                item_type = item.get(\"type\")\n",
    "                if item_type == \"image\" and item.get(\"image\") is not None:\n",
    "                    images.append(item[\"image\"])\n",
    "                elif item_type == \"audio\" and item.get(\"audio\") is not None:\n",
    "                    audio.append(item[\"audio\"])\n",
    "                elif item_type == \"video\" and item.get(\"video\") is not None:\n",
    "                    videos.append(item[\"video\"])\n",
    "        return (audio or None, images or None, videos or None)\n",
    "\n",
    "    def generate(self, messages: Sequence[dict]) -> str:\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        audios, images, videos = self._collect_media(messages)\n",
    "        inputs = self.processor(\n",
    "            text=text,\n",
    "            audio=audios,\n",
    "            images=images,\n",
    "            videos=videos,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            use_audio_in_video=self.use_audio_in_video,\n",
    "        )\n",
    "        inputs = inputs.to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            return_audio=False,\n",
    "            thinker_return_dict_in_generate=True,\n",
    "            use_audio_in_video=self.use_audio_in_video,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            temperature=self.temperature,\n",
    "            do_sample=self.temperature > 0,\n",
    "        )\n",
    "        sequences = getattr(outputs, \"sequences\", outputs)\n",
    "        offset = inputs[\"input_ids\"].shape[1]\n",
    "        text = self.processor.batch_decode(\n",
    "            sequences[:, offset:],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"High level helper for running retrieval augmented generation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rag_config: Optional[RAGConfig] = None,\n",
    "        train_config: Optional[TrainConfig] = None,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        model_key: Optional[str] = None,\n",
    "        use_finetuned: bool = True,\n",
    "    ) -> None:\n",
    "        self.rag_config = rag_config or RAGConfig()\n",
    "        self.train_config = train_config or TrainConfig()\n",
    "        self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "        self.model_key = model_key or MODEL_OPTIONS[0].key\n",
    "        self.model_option = self._resolve_model_option(self.model_key)\n",
    "        self.use_finetuned = use_finetuned\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.rag_config.embedding_model_name)\n",
    "        self.vector_store = self._load_vector_store(self.rag_config.vector_db_path)\n",
    "        base_model, adapter_path = self._resolve_model_sources()\n",
    "        if self.model_option.multimodal:\n",
    "            self.generator = QwenOmniGenerator(base_model, adapter_path=adapter_path)\n",
    "        else:\n",
    "            self.generator = LocalHFGenerator(base_model, adapter_path=adapter_path)\n",
    "\n",
    "    def _resolve_model_option(self, model_key: str) -> ModelOption:\n",
    "        try:\n",
    "            return MODEL_OPTION_MAP[model_key]\n",
    "        except KeyError as exc:  # pragma: no cover - defensive programming\n",
    "            raise ValueError(f\"Unknown model selection: {model_key}\") from exc\n",
    "\n",
    "    def _resolve_model_sources(self) -> Tuple[str, Optional[str]]:\n",
    "        base_model = self.model_option.model_name\n",
    "        if not self.use_finetuned:\n",
    "            return base_model, None\n",
    "\n",
    "        if self.model_option.key != \"qwen3-omni-30b\":\n",
    "            LOGGER.warning(\n",
    "                \"Fine-tuned adapters are currently only supported for Qwen3-Omni. Continuing with base %s.\",\n",
    "                base_model,\n",
    "            )\n",
    "            return base_model, None\n",
    "\n",
    "        adapter_dir = Path(self.train_config.output_dir)\n",
    "        if not adapter_dir.exists():\n",
    "            LOGGER.warning(\n",
    "                \"Requested fine-tuned model but no adapters found at %s. Continuing with base model.\",\n",
    "                adapter_dir,\n",
    "            )\n",
    "            return base_model, None\n",
    "\n",
    "        if not self._contains_adapter_weights(adapter_dir):\n",
    "            LOGGER.warning(\n",
    "                \"Adapter directory %s does not contain LoRA weights. Continuing with base model.\",\n",
    "                adapter_dir,\n",
    "            )\n",
    "            return base_model, None\n",
    "\n",
    "        return base_model, str(adapter_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def _contains_adapter_weights(adapter_dir: Path) -> bool:\n",
    "        expected_config = adapter_dir / \"adapter_config.json\"\n",
    "        if expected_config.exists():\n",
    "            return True\n",
    "        # Accept common safetensors/bin outputs\n",
    "        for pattern in (\"adapter_model.bin\", \"adapter_model.safetensors\"):\n",
    "            if (adapter_dir / pattern).exists():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _load_vector_store(self, path: Path | str) -> Optional[FAISS]:\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            LOGGER.warning(\n",
    "                \"Vector store not found at %s. Responses will be generated without retrieval until you run the ingestion pipeline.\",\n",
    "                path,\n",
    "            )\n",
    "            return None\n",
    "        return FAISS.load_local(\n",
    "            str(path),\n",
    "            embeddings=self.embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "    def _format_context(self, documents: Iterable[Document]) -> str:\n",
    "        context_blocks: List[str] = []\n",
    "        for idx, doc in enumerate(documents, start=1):\n",
    "            metadata = doc.metadata\n",
    "            source = metadata.get(\"source\", \"unknown\")\n",
    "            block = dedent(\n",
    "                f\"\"\"\n",
    "                [Document {idx} | Source: {source}]\n",
    "                {doc.page_content.strip()}\n",
    "                \"\"\"\n",
    "            ).strip()\n",
    "            context_blocks.append(block)\n",
    "        return \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    def _build_text_history(self, history: Optional[Sequence[Tuple[str, str]]]) -> str:\n",
    "        if not history:\n",
    "            return \"\"\n",
    "        turns: List[str] = []\n",
    "        for user_text, assistant_text in history:\n",
    "            if user_text:\n",
    "                turns.append(\n",
    "                    dedent(\n",
    "                        f\"\"\"\n",
    "                        <|im_start|>user\n",
    "                        {user_text}\n",
    "                        <|im_end|>\n",
    "                        \"\"\"\n",
    "                    ).strip()\n",
    "                )\n",
    "            if assistant_text:\n",
    "                turns.append(\n",
    "                    dedent(\n",
    "                        f\"\"\"\n",
    "                        <|im_start|>assistant\n",
    "                        {assistant_text}\n",
    "                        <|im_end|>\n",
    "                        \"\"\"\n",
    "                    ).strip()\n",
    "                )\n",
    "        return \"\\n\".join(turns)\n",
    "\n",
    "    def build_prompt(\n",
    "        self,\n",
    "        question: str,\n",
    "        documents: List[Document],\n",
    "        history: Optional[Sequence[Tuple[str, str]]] = None,\n",
    "    ) -> str:\n",
    "        context = self._format_context(documents)\n",
    "        history_block = self._build_text_history(history)\n",
    "        prompt_blocks = [\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                <|im_start|>system\n",
    "                {self.system_prompt}\n",
    "                <|im_end|>\n",
    "                \"\"\"\n",
    "            ).strip()\n",
    "        ]\n",
    "        if history_block:\n",
    "            prompt_blocks.append(history_block)\n",
    "        prompt_blocks.append(\n",
    "            dedent(\n",
    "                f\"\"\"\n",
    "                <|im_start|>user\n",
    "                Question: {question}\n",
    "\n",
    "                Use the following context to ground your answer:\n",
    "                {context}\n",
    "                <|im_end|>\n",
    "                <|im_start|>assistant\n",
    "                \"\"\"\n",
    "            ).strip()\n",
    "        )\n",
    "        return \"\\n\".join(prompt_blocks)\n",
    "\n",
    "    def _build_multimodal_messages(\n",
    "        self,\n",
    "        question: str,\n",
    "        documents: List[Document],\n",
    "        media: MediaInput,\n",
    "        history: Optional[Sequence[Tuple[str, str]]],\n",
    "    ) -> List[dict]:\n",
    "        messages: List[dict] = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": self.system_prompt}],\n",
    "            }\n",
    "        ]\n",
    "        if documents:\n",
    "            context = self._format_context(documents)\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": (\n",
    "                                \"Kasetsart University supporting material:\\n\"\n",
    "                                f\"{context}\"\n",
    "                            ),\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        if history:\n",
    "            for user_turn, assistant_turn in history:\n",
    "                if user_turn:\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": user_turn}],\n",
    "                        }\n",
    "                    )\n",
    "                if assistant_turn:\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": assistant_turn}],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        user_content: List[dict] = []\n",
    "        for image_path in media.images:\n",
    "            user_content.append({\"type\": \"image\", \"image\": image_path})\n",
    "        for audio_path in media.audio:\n",
    "            user_content.append({\"type\": \"audio\", \"audio\": audio_path})\n",
    "        for video_path in media.video:\n",
    "            user_content.append({\"type\": \"video\", \"video\": video_path})\n",
    "\n",
    "        request_text = question.strip() or (\n",
    "            \"Please analyse the uploaded media and explain how it relates to Kasetsart University's \"\n",
    "            \"Computer Science programme.\"\n",
    "        )\n",
    "        user_content.append({\"type\": \"text\", \"text\": request_text})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        return messages\n",
    "\n",
    "    def answer(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: Optional[int] = None,\n",
    "        media: Optional[MediaInput] = None,\n",
    "        history: Optional[Sequence[Tuple[str, str]]] = None,\n",
    "    ) -> str:\n",
    "        media = media or MediaInput()\n",
    "        question_text = question.strip()\n",
    "        if not question_text and media.is_empty():\n",
    "            return \"Please provide a question or upload audio, image, or video content.\"\n",
    "\n",
    "        documents: List[Document] = []\n",
    "        if question_text and self.vector_store is not None:\n",
    "            top_k = top_k or self.rag_config.max_retrieval_docs\n",
    "            retriever = self.vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "            documents = retriever.get_relevant_documents(question_text)\n",
    "            if not documents and not self.model_option.multimodal:\n",
    "                return \"I could not find supporting documents for that question.\"\n",
    "        elif question_text and self.vector_store is None:\n",
    "            LOGGER.warning(\"Vector store unavailable; continuing without retrieved context.\")\n",
    "\n",
    "        if self.model_option.multimodal:\n",
    "            messages = self._build_multimodal_messages(question_text, documents, media, history)\n",
    "            return self.generator.generate(messages)\n",
    "\n",
    "        prompt = self.build_prompt(question_text, documents, history)\n",
    "        return self.generator.generate(prompt)\n",
    "\n",
    "\n",
    "__all__ = [\"RAGPipeline\", \"LocalHFGenerator\", \"QwenOmniGenerator\", \"MediaInput\", \"DEFAULT_SYSTEM_PROMPT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Supervised fine-tuning pipeline for Qwen3-Omni-30B.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "try:  # Optional imports used only during training\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "except ImportError as exc:  # pragma: no cover - only raised when deps missing\n",
    "    raise ImportError(\n",
    "        \"peft is required for LoRA training. Install with `pip install peft`.\"\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "ChatMessages = List[Dict[str, str]]\n",
    "\n",
    "\n",
    "def _normalise_sample(example: Dict[str, Any]) -> str:\n",
    "    \"\"\"Normalise a dataset row into a chat-style plain text sample.\"\"\"\n",
    "\n",
    "    if \"messages\" in example:\n",
    "        messages: ChatMessages = example[\"messages\"]\n",
    "        return messages_to_text(messages)\n",
    "\n",
    "    if {\"instruction\", \"response\"}.issubset(example):\n",
    "        system_prompt = example.get(\"system\", \"You are a helpful assistant.\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "        ]\n",
    "        if example.get(\"input\"):\n",
    "            messages.insert(2, {\"role\": \"user\", \"content\": example[\"input\"]})\n",
    "        return messages_to_text(messages)\n",
    "\n",
    "    text_field = example.get(\"text\")\n",
    "    if text_field:\n",
    "        return str(text_field)\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unsupported dataset schema. Expected `messages`, `text` or `instruction`/`response` columns.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def messages_to_text(messages: ChatMessages) -> str:\n",
    "    \"\"\"Convert chat messages into model-ready text using the tokenizer template.\"\"\"\n",
    "\n",
    "    formatted: List[str] = []\n",
    "    for message in messages:\n",
    "        role = message.get(\"role\", \"user\")\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        formatted.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "    formatted.append(\"<|im_start|>assistant\\n\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "class SupervisedFineTuner:\n",
    "    \"\"\"LoRA supervised fine-tuning helper for the Qwen 3 Omni family.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[TrainConfig] = None) -> None:\n",
    "        self.config = config or TrainConfig()\n",
    "        set_seed(self.config.seed)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_name, trust_remote_code=True\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model: Optional[torch.nn.Module] = None\n",
    "        self.train_dataset: Optional[Dataset] = None\n",
    "        self.eval_dataset: Optional[Dataset] = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Dataset utilities\n",
    "    # ------------------------------------------------------------------\n",
    "    def prepare_datasets(\n",
    "        self, eval_split: Optional[float] = 0.05, streaming: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"Load datasets, normalise formatting and tokenize them.\"\"\"\n",
    "\n",
    "        dataset_path = self.config.dataset_path\n",
    "        data_files: Dict[str, str]\n",
    "        path = Path(dataset_path)\n",
    "        if path.is_dir():\n",
    "            data_files = {\"train\": str(path / \"train.jsonl\")}\n",
    "        else:\n",
    "            data_files = {\"train\": str(path)}\n",
    "\n",
    "        if streaming:\n",
    "            dataset = load_dataset(\"json\", data_files=data_files, streaming=True)[\n",
    "                \"train\"\n",
    "            ]\n",
    "            raise NotImplementedError(\"Streaming datasets are not supported in this release.\")\n",
    "\n",
    "        dataset_dict = load_dataset(\"json\", data_files=data_files)\n",
    "        field_name = self.config.dataset_text_field\n",
    "        train_ds = dataset_dict[\"train\"].map(\n",
    "            lambda example: {field_name: _normalise_sample(example)}\n",
    "        )\n",
    "        if eval_split:\n",
    "            split = train_ds.train_test_split(test_size=eval_split, seed=self.config.seed)\n",
    "            self.train_dataset = self._tokenize(split[\"train\"])\n",
    "            self.eval_dataset = self._tokenize(split[\"test\"])\n",
    "        else:\n",
    "            self.train_dataset = self._tokenize(train_ds)\n",
    "            self.eval_dataset = None\n",
    "\n",
    "    def _tokenize(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Tokenize dataset into model inputs.\"\"\"\n",
    "\n",
    "        def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            return self.tokenizer(\n",
    "                examples[self.config.dataset_text_field],\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length,\n",
    "            )\n",
    "\n",
    "        return dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[self.config.dataset_text_field],\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Model utilities\n",
    "    # ------------------------------------------------------------------\n",
    "    def _load_model(self) -> None:\n",
    "        torch_dtype = torch.bfloat16 if self.config.bf16 else torch.float16\n",
    "        quantization_config: Dict[str, Any] = {}\n",
    "        if self.config.load_in_4bit:\n",
    "            quantization_config = {\n",
    "                \"load_in_4bit\": True,\n",
    "                \"bnb_4bit_compute_dtype\": torch_dtype,\n",
    "                \"bnb_4bit_quant_type\": \"nf4\",\n",
    "                \"bnb_4bit_use_double_quant\": True,\n",
    "            }\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            **quantization_config,\n",
    "        )\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        if self.config.use_gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Training orchestration\n",
    "    # ------------------------------------------------------------------\n",
    "    def train(self) -> None:\n",
    "        if self.train_dataset is None:\n",
    "            self.prepare_datasets()\n",
    "        if self.model is None:\n",
    "            self._load_model()\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            bf16=self.config.bf16,\n",
    "            evaluation_strategy=\"steps\" if self.eval_dataset is not None else \"no\",\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            report_to=[\"tensorboard\"],\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            self.tokenizer, mlm=False, pad_to_multiple_of=8\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.config.output_dir)\n",
    "        with open(Path(self.config.output_dir) / \"train_config.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(asdict(self.config), fp, indent=2)\n",
    "\n",
    "\n",
    "__all__ = [\"SupervisedFineTuner\", \"TrainConfig\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gradio chat interface for the KUx assistant.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass, replace\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Sequence, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_LABEL_TO_KEY = {option.label: option.key for option in MODEL_OPTIONS}\n",
    "MODEL_KEY_TO_LABEL = {option.key: option.label for option in MODEL_OPTIONS}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LaunchState:\n",
    "    \"\"\"Runtime configuration supplied when launching the Gradio app.\"\"\"\n",
    "\n",
    "    vector_db_path: Optional[str] = None\n",
    "    adapter_dir: Optional[str] = None\n",
    "    default_model_key: str = MODEL_OPTIONS[0].key\n",
    "    default_system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "_LAUNCH_STATE = LaunchState()\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=8)\n",
    "def _load_pipeline_cached(\n",
    "    model_key: str,\n",
    "    use_finetuned: bool,\n",
    "    system_prompt: str,\n",
    "    vector_db_path: Optional[str],\n",
    "    adapter_dir: Optional[str],\n",
    ") -> RAGPipeline:\n",
    "    LOGGER.info(\n",
    "        \"Initialising RAG pipeline (model=%s, finetuned=%s)\",\n",
    "        model_key,\n",
    "        use_finetuned,\n",
    "    )\n",
    "    default_rag = RAGConfig()\n",
    "    rag_config = RAGConfig(\n",
    "        vector_db_path=Path(vector_db_path) if vector_db_path else default_rag.vector_db_path\n",
    "    )\n",
    "    default_train = TrainConfig()\n",
    "    train_config = TrainConfig(output_dir=adapter_dir if adapter_dir else default_train.output_dir)\n",
    "    return RAGPipeline(\n",
    "        rag_config=rag_config,\n",
    "        train_config=train_config,\n",
    "        system_prompt=system_prompt,\n",
    "        model_key=model_key,\n",
    "        use_finetuned=use_finetuned,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_pipeline(model_key: str, use_finetuned: bool, system_prompt: str) -> RAGPipeline:\n",
    "    base_prompt = _LAUNCH_STATE.default_system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "    prompt = system_prompt.strip() if system_prompt and system_prompt.strip() else base_prompt\n",
    "    return _load_pipeline_cached(\n",
    "        model_key,\n",
    "        use_finetuned,\n",
    "        prompt,\n",
    "        _LAUNCH_STATE.vector_db_path,\n",
    "        _LAUNCH_STATE.adapter_dir,\n",
    "    )\n",
    "\n",
    "\n",
    "def _extract_paths(payload: Any) -> List[str]:\n",
    "    if not payload:\n",
    "        return []\n",
    "    if isinstance(payload, (str, Path)):\n",
    "        return [str(payload)]\n",
    "    items: Sequence[Any]\n",
    "    if isinstance(payload, Sequence):\n",
    "        items = payload\n",
    "    else:  # single temp file or dict\n",
    "        items = [payload]\n",
    "    paths: List[str] = []\n",
    "    for item in items:\n",
    "        if not item:\n",
    "            continue\n",
    "        if isinstance(item, (str, Path)):\n",
    "            paths.append(str(item))\n",
    "        elif isinstance(item, dict):\n",
    "            for key in (\"path\", \"name\"):\n",
    "                value = item.get(key)\n",
    "                if value:\n",
    "                    paths.append(str(value))\n",
    "                    break\n",
    "        else:\n",
    "            path = getattr(item, \"name\", None)\n",
    "            if path:\n",
    "                paths.append(str(path))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def _format_user_display(message: str, media: MediaInput) -> str:\n",
    "    parts: List[str] = []\n",
    "    text = message.strip()\n",
    "    if text:\n",
    "        parts.append(text)\n",
    "    for label, files in (\n",
    "        (\"Image\", media.images),\n",
    "        (\"Audio\", media.audio),\n",
    "        (\"Video\", media.video),\n",
    "    ):\n",
    "        for file_path in files:\n",
    "            parts.append(f\"[{label}] {Path(file_path).name}\")\n",
    "    return \"\\n\".join(parts) if parts else \"[No user content]\"\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    history: List[Tuple[str, str]],\n",
    "    model_label: str,\n",
    "    use_finetuned: bool,\n",
    "    system_prompt: str,\n",
    "    image_payload: Any,\n",
    "    audio_payload: Any,\n",
    "    video_payload: Any,\n",
    ") -> Tuple[List[Tuple[str, str]], gr.Textbox, gr.File, gr.File, gr.File]:\n",
    "    model_key = MODEL_LABEL_TO_KEY.get(model_label, MODEL_OPTIONS[0].key)\n",
    "    pipeline = load_pipeline(model_key, use_finetuned, system_prompt)\n",
    "    media = MediaInput.from_payload(\n",
    "        images=_extract_paths(image_payload),\n",
    "        audio=_extract_paths(audio_payload),\n",
    "        video=_extract_paths(video_payload),\n",
    "    )\n",
    "    display_text = _format_user_display(message, media)\n",
    "    prompt_history = [(user, bot) for user, bot in history if user or bot]\n",
    "    answer = pipeline.answer(message, media=media, history=prompt_history)\n",
    "    updated_history = history + [(display_text, answer)]\n",
    "    return (\n",
    "        updated_history,\n",
    "        gr.Textbox.update(value=\"\"),\n",
    "        gr.File.update(value=None),\n",
    "        gr.File.update(value=None),\n",
    "        gr.File.update(value=None),\n",
    "    )\n",
    "\n",
    "\n",
    "def launch(\n",
    "    *,\n",
    "    vector_db_path: Optional[str] = None,\n",
    "    adapter_dir: Optional[str] = None,\n",
    "    default_model_key: Optional[str] = None,\n",
    "    default_system_prompt: Optional[str] = None,\n",
    "    share: bool = False,\n",
    "    server_name: str = \"0.0.0.0\",\n",
    "    server_port: int = 7860,\n",
    "    preload_default: bool = True,\n",
    ") -> None:\n",
    "    global _LAUNCH_STATE\n",
    "    desired_model_key = default_model_key or MODEL_OPTIONS[0].key\n",
    "    _LAUNCH_STATE = replace(\n",
    "        _LAUNCH_STATE,\n",
    "        vector_db_path=str(vector_db_path) if vector_db_path else None,\n",
    "        adapter_dir=str(adapter_dir) if adapter_dir else None,\n",
    "        default_model_key=(\n",
    "            desired_model_key if desired_model_key in MODEL_KEY_TO_LABEL else MODEL_OPTIONS[0].key\n",
    "        ),\n",
    "        default_system_prompt=(\n",
    "            default_system_prompt.strip()\n",
    "            if default_system_prompt and default_system_prompt.strip()\n",
    "            else DEFAULT_SYSTEM_PROMPT\n",
    "        ),\n",
    "    )\n",
    "    system_prompt_default = _LAUNCH_STATE.default_system_prompt or DEFAULT_SYSTEM_PROMPT\n",
    "    _load_pipeline_cached.cache_clear()\n",
    "    if preload_default:\n",
    "        LOGGER.info(\n",
    "            \"Preloading default model %s (finetuned=%s)\",\n",
    "            _LAUNCH_STATE.default_model_key,\n",
    "            True,\n",
    "        )\n",
    "        try:\n",
    "            load_pipeline(_LAUNCH_STATE.default_model_key, True, system_prompt_default)\n",
    "        except Exception as exc:  # pragma: no cover - defensive for runtime errors\n",
    "            LOGGER.exception(\"Failed to preload the default model: %s\", exc)\n",
    "            raise\n",
    "    description = (\n",
    "        \"KUx is a retrieval-augmented assistant for Kasetsart University Computer Science students.\"\n",
    "    )\n",
    "    theme = gr.themes.Default(primary_hue=gr.themes.colors.green)\n",
    "    default_model_label = MODEL_KEY_TO_LABEL.get(_LAUNCH_STATE.default_model_key, MODEL_OPTIONS[0].label)\n",
    "    with gr.Blocks(theme=theme) as demo:\n",
    "        gr.Markdown(\n",
    "            \"# KUx \u2013 Kasetsart CS Assistant\\n\"\n",
    "            f\"<span style='color: #0f5132'>{description}</span>\",\n",
    "            elem_id=\"kux-header\",\n",
    "        )\n",
    "        chatbot = gr.Chatbot(label=\"Conversation\", height=520)\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                message_box = gr.Textbox(\n",
    "                    label=\"Ask KUx\",\n",
    "                    placeholder=\"Ask a question or leave blank to analyse uploaded media\",\n",
    "                    lines=4,\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"Clear conversation\", variant=\"secondary\")\n",
    "            with gr.Column(scale=2):\n",
    "                image_files = gr.File(\n",
    "                    label=\"Upload images (OCR, object grounding, image math)\",\n",
    "                    file_types=[\"image\"],\n",
    "                    file_count=\"multiple\",\n",
    "                )\n",
    "                audio_files = gr.File(\n",
    "                    label=\"Upload audio (speech recognition, translation, captioning)\",\n",
    "                    file_types=[\"audio\"],\n",
    "                    file_count=\"multiple\",\n",
    "                )\n",
    "                video_files = gr.File(\n",
    "                    label=\"Upload videos (audio-visual QA/interactions)\",\n",
    "                    file_types=[\"video\"],\n",
    "                    file_count=\"multiple\",\n",
    "                )\n",
    "        with gr.Accordion(\"Assistant settings\", open=False):\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                label=\"Base model\",\n",
    "                choices=list(MODEL_LABEL_TO_KEY.keys()),\n",
    "                value=default_model_label,\n",
    "                interactive=True,\n",
    "            )\n",
    "            finetune_checkbox = gr.Checkbox(\n",
    "                label=\"Use fine-tuned adapters (LoRA)\",\n",
    "                value=True,\n",
    "                interactive=True,\n",
    "            )\n",
    "            system_prompt_box = gr.Textbox(\n",
    "                label=\"System prompt\",\n",
    "                value=system_prompt_default,\n",
    "                lines=4,\n",
    "                interactive=True,\n",
    "            )\n",
    "\n",
    "        send_btn.click(\n",
    "            fn=respond,\n",
    "            inputs=[\n",
    "                message_box,\n",
    "                chatbot,\n",
    "                model_dropdown,\n",
    "                finetune_checkbox,\n",
    "                system_prompt_box,\n",
    "                image_files,\n",
    "                audio_files,\n",
    "                video_files,\n",
    "            ],\n",
    "            outputs=[chatbot, message_box, image_files, audio_files, video_files],\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\n",
    "                [],\n",
    "                gr.Textbox.update(value=\"\"),\n",
    "                gr.File.update(value=None),\n",
    "                gr.File.update(value=None),\n",
    "                gr.File.update(value=None),\n",
    "            ),\n",
    "            inputs=None,\n",
    "            outputs=[chatbot, message_box, image_files, audio_files, video_files],\n",
    "        )\n",
    "\n",
    "    demo.queue().launch(server_name=server_name, server_port=server_port, share=share)\n",
    "\n",
    "\n",
    "__all__ = [\"launch\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: crawl official Kasetsart CS sources\n",
    "\n",
    "Enable `enable_crawl` in the configuration cell to fetch fresh pages. Outputs are saved under `data/crawled/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_crawl']:\n",
    "    crawl_seed_urls = CONFIG['crawl_seed_urls']\n",
    "    if not crawl_seed_urls:\n",
    "        raise ValueError('No seed URLs specified. Update CONFIG[\"crawl_seed_urls\"].')\n",
    "\n",
    "    crawler_config = CrawlerConfig(\n",
    "        max_depth=CONFIG.get('crawl_max_depth', 1),\n",
    "        max_pages=CONFIG.get('crawl_max_pages', 10),\n",
    "        cache_dir=DATA_DIR / 'crawled_cache',\n",
    "    )\n",
    "    crawler = SiteCrawler(crawler_config)\n",
    "    crawled = crawler.crawl(crawl_seed_urls)\n",
    "    output_dir = DATA_DIR / 'crawled'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, (url, text) in enumerate(crawled.items(), start=1):\n",
    "        target = output_dir / f'page_{idx:03d}.txt'\n",
    "        target.write_text(text, encoding='utf-8')\n",
    "        print(f'Saved {target.relative_to(PROJECT_ROOT)} \u2190 {url}')\n",
    "else:\n",
    "    print('Skipping crawl (CONFIG[\"enable_crawl\"] is False).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build or refresh the FAISS vector store\n",
    "\n",
    "When `enable_ingest` is `True`, the ingestor walks the directories listed in `ingest_sources` (relative to the repo root), rebuilds the embeddings, and saves them to the configured vector store directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_ingest']:\n",
    "    sources = [PROJECT_ROOT / Path(path) for path in CONFIG.get('ingest_sources', [])]\n",
    "    resolved_sources = [str(path) for path in sources if path.exists()]\n",
    "    if not resolved_sources:\n",
    "        raise RuntimeError('No sources found. Add PDFs/CSVs/text files under data/ or update CONFIG[\"ingest_sources\"].')\n",
    "\n",
    "    rag_config = RAGConfig(vector_db_path=VECTOR_DB_DIR)\n",
    "    ingestor = DocumentIngestor(rag_config)\n",
    "    vector_store = ingestor.ingest(resolved_sources)\n",
    "    print('Vector store saved to', VECTOR_DB_DIR)\n",
    "else:\n",
    "    print('Skipping ingestion (CONFIG[\"enable_ingest\"] is False). Ensure a vector store exists before launching the chatbot.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: fine-tune Qwen with LoRA adapters\n",
    "\n",
    "Enable `enable_finetune` and provide a chat-style dataset at the path specified by `finetune_dataset` (relative to the repo root). The adapters are written to the directory configured in `adapter_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_finetune']:\n",
    "    dataset_path = PROJECT_ROOT / CONFIG['finetune_dataset']\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f'Dataset not found at {dataset_path}. Upload your training data or update CONFIG[\"finetune_dataset\"].')\n",
    "\n",
    "    train_config = TrainConfig(\n",
    "        dataset_path=str(dataset_path),\n",
    "        output_dir=str(ADAPTER_DIR),\n",
    "        num_train_epochs=CONFIG.get('finetune_epochs', 2),\n",
    "    )\n",
    "    trainer = SupervisedFineTuner(train_config)\n",
    "    trainer.prepare_datasets()\n",
    "    trainer.train()\n",
    "else:\n",
    "    print('Skipping fine-tuning (CONFIG[\"enable_finetune\"] is False).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch the multimodal KUx chatbot\n",
    "\n",
    "This cell blocks while the Gradio app is running. Gradio prints both the local and public share URLs\u2014open the share URL in a new tab to chat with KUx. Stop the cell when you want to shut down the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = {option.key for option in MODEL_OPTIONS}\n",
    "default_model_key = CONFIG.get('default_model_key') or MODEL_OPTIONS[0].key\n",
    "if default_model_key not in model_keys:\n",
    "    raise ValueError(f\"Unknown model key '{default_model_key}'. Choose from: {sorted(model_keys)}\")\n",
    "\n",
    "system_prompt = CONFIG.get('default_system_prompt') or DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "launch(\n",
    "    vector_db_path=CONFIG['vector_db_dir'],\n",
    "    adapter_dir=CONFIG['adapter_dir'],\n",
    "    default_model_key=default_model_key,\n",
    "    default_system_prompt=system_prompt,\n",
    "    share=CONFIG.get('launch_share', False),\n",
    "    preload_default=CONFIG.get('launch_preload', True),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}